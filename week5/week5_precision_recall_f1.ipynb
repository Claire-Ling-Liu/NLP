{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sys.argv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz introduction:\n",
    "\n",
    "...\n",
    "\n",
    "You should be able to launch your script like so:\n",
    "\n",
    "eval.py wsj22-24.tag wsj22-24.guess\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sys.argv is a list in Python, which contains the command-line arguments passed to the script. \n",
    "\n",
    "If you are gonna work with command line arguments, you probably want to \n",
    "use sys.argv. \n",
    "\n",
    "To use sys.argv, you will first have to import the sys module. \n",
    "\n",
    "sys.argv[0] is the program ie. the script name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Classification: Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = (number of correct classifications) / (total number of classifications made)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall & Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall and Precision are more fine-grained measures of classification accuracy and answer different questions. \n",
    "\n",
    "These measures are usually used for two-way classifications—a yes/no or positive/non-positive classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall: how many of the relevant items were returned? -- how complete the search results are\n",
    "    \n",
    "### Precision: how many of the returned items were relevant? -- how useful the search results are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/img1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall = TP / actual yes\n",
    "\n",
    "100/105 = 0.95\n",
    "\n",
    "\n",
    "precision = TP / predicted yes\n",
    "\n",
    "100/110 = 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score = 2PR / (P+R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score is the harmonic average of precision (P) and recall (R).\n",
    "\n",
    "It is common to report all three numbers—P, R, and F—to provide a clear overview of the performance of a two-way classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall, Precision and F1 for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of a tagger as doing a binary classification for each tag. Then we can calculate precision and recall per tag also (not just give overall accuracy as in the quiz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,\n",
    "\n",
    "\"Natural language processing is a field of computer science\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goldstandard:\n",
    "Natural language processing is a field of computer science\n",
    "JJ        NN         NN    VBZ DT NN   IN   NN       NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tagger's prediction:\n",
    "Natural language processing is a field of computer science\n",
    "JJ        NN       VBG     VBZ DT NN   IN   NN       NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tag NN:\n",
    "\n",
    "precision = (# of correct NN) / (# of tagged NN) = 4 / 4 = 1\n",
    "\n",
    "recall = (# of correct NN) / (# of gold NN) = 4 / 5 = 0.8\n",
    "\n",
    "F1 = 2PR/(P+R) = 2*1*0.8 / (1+0.8) = 0.89"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Exercise:\n",
    "\n",
    "Read each of the file for quiz4 into a list of (word, tag) tuples. (Empty lines should be included.)\n",
    "Define a function that takes as input the two list of tuples and a part-of-speec tag, and returns the precision, recall and F1-score for the pos tag of the tagger. Keep two decimals.\n",
    "\n",
    "\n",
    "def PRF(gold, guess, pos):\n",
    "    ...\n",
    "    return p, r, f\n",
    "    \n",
    "    \n",
    "For example, for NN, the returned precision, recall, and f1 should be 96.57, 96.32, 96.44 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
